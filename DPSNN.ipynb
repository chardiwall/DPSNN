{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chardiwall/DPSNN/blob/main/DPSNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TwYDVep0-oO"
      },
      "source": [
        "# Project Discription\n",
        "**Title:** DPSNN (A Differentially Private Spiking Neural Network with Temporal Enhanced Polling).\n",
        "\n",
        "**Objectives:** Implementation of the [paper](https://arxiv.org/pdf/2205.12718.pdf).\n",
        "\n",
        "**Discription:** The project is set in the context of the growing field of social robotics, which involves the deployment of robots in human-centric environments. These environments can range from healthcare facilities and educational institutions to customer service and domestic settings. The unique aspect of this project lies in its focus on privacy-preserving mechanisms, an increasingly critical concern in today's data-driven world.\n",
        "\n",
        "\n",
        "[GitHub Link](https://github.com/chardiwall/DPSNN)\n",
        "\n",
        "---\n",
        "## Frameworks\n",
        "SNN framwork: [SNNTorch](https://snntorch.readthedocs.io/en/latest/)\n",
        "\n",
        "DP framwork: [Opacus](https://github.com/pytorch/opacus)\n",
        "\n",
        "---\n",
        "## Datasets\n",
        "\n",
        "| Static      | Neuromorphic |\n",
        "--------------|----------------\n",
        "| [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html)      |[N-MNIST](https://www.garrickorchard.com/datasets/n-mnist#h.p_ID_38) |\n",
        "| [MNIST](https://yann.lecun.com/exdb/mnist/)      | [CIFAR10-DVS](https://figshare.com/articles/dataset/CIFAR10-DVS_New/4724671/2) |\n",
        "| [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist)      |   |\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFHM-9gk2Bug"
      },
      "source": [
        "# Loading Essential Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEGy77YL3Aug",
        "outputId": "0539944c-5cc0-4a3f-d667-1780dd8754e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.0/109.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.2/76.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# !pip install -q opacus\n",
        "!pip install -q snntorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGWhf2je6V0i",
        "outputId": "12388849-2210-4df1-926b-4b9b5e6ab489"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at drive; to attempt to forcibly remount, call drive.mount(\"drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mounting Google Drive as drive\n",
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3pG9qP43SUH",
        "outputId": "17a6280f-39ae-44e0-be18-3b8a3cc03cf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-b9b3fb4038d0>:27: DeprecationWarning: The module snntorch.backprop will be deprecated in  a future release. Writing out your own training loop will lead to substantially faster performance.\n",
            "  from snntorch import utils, backprop, surrogate\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import itertools\n",
        "from copy import copy\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from zipfile import ZipFile\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from torchsummary import summary\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "import snntorch as snn\n",
        "from snntorch import functional as SF\n",
        "from snntorch import spikeplot as splt\n",
        "from snntorch import utils, backprop, surrogate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I20KiteW3n1F"
      },
      "source": [
        "# Unzipping and Loading Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xV4NAFuU3sAr"
      },
      "source": [
        "## Neuromorphic Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qBkFa3i3y3d"
      },
      "outputs": [],
      "source": [
        "# N-MNIST\n",
        "\n",
        "# unzipping the data\n",
        "base_dir_path = '/content/drive/MyDrive/Colab Notebooks/DPSNN/N-MNIST/'\n",
        "train_zip_name = 'Copy of Train'\n",
        "test_zip_name = 'Copy of Test'\n",
        "\n",
        "for zip_file in [train_zip_name, test_zip_name]:\n",
        "  zip_path = os.path.join(base_dir_path, zip_file+'.zip')\n",
        "\n",
        "  with ZipFile(zip_path, 'r') as zip_ref:\n",
        "    # extraction_path = os.path.join(base_dir_path, zip_file)\n",
        "    # if not (os.path.exists(extraction_path)):\n",
        "    #   os.mkdir(extraction_path)\n",
        "    zip_ref.extractall(base_dir_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEntHGB87KkU"
      },
      "outputs": [],
      "source": [
        "# CIFAR10-DVS\n",
        "base_dir_path = '/content/drive/MyDrive/Colab Notebooks/DPSNN/CIFAR10-DVS'\n",
        "file_name = 'CIFAR10-DVS.zip'\n",
        "\n",
        "with ZipFile(os.path.join(base_dir_path, file_name), 'r') as zip_ref:\n",
        "  zip_ref.extractall(base_dir_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRVHqqevWViG"
      },
      "source": [
        "## Static Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BoQr-tHJtQt0"
      },
      "outputs": [],
      "source": [
        "def fetch_static_data(name = 'mnist', transform = None, batch_size = 64, shuffle = True):\n",
        "  \"\"\"\n",
        "    Fetches static datasets such as MNIST, Fashion MNIST, and CIFAR10.\n",
        "\n",
        "    Args:\n",
        "    - name (str): Name of the dataset ('mnist', 'fashion-mnist', 'cifar10').\n",
        "    - transform (callable, optional): A function/transform to apply to the data.\n",
        "    - batch_size (int, optional): Number of samples in each batch.\n",
        "    - shuffle (bool, optional): Set to True to shuffle the data.\n",
        "\n",
        "    Returns:\n",
        "    - train_loader (DataLoader): DataLoader for the training set.\n",
        "    - test_loader (DataLoader): DataLoader for the test set.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "  # MNIST\n",
        "  if name.lower() == 'mnist':\n",
        "    train = datasets.MNIST(root = '.', train=True, transform=transform, download=True)\n",
        "    test = datasets.MNIST(root = '.', train=False, transform=transform, download=True)\n",
        "\n",
        "  # Fashion MNIST\n",
        "  elif name.lower() == 'fashion-mnist':\n",
        "    train = datasets.FashionMNIST(root = '.', train=True, transform=transform, download=True)\n",
        "    test = datasets.FashionMNIST(root = '.', train=False, transform=transform, download=True)\n",
        "\n",
        "  # CIFAR10\n",
        "  elif name.lower() == 'cifar-10':\n",
        "    train = datasets.CIFAR10('.', train = True, transform = transform, download = True)\n",
        "    test = datasets.CIFAR10('.', train = False, transform = transform, download = True)\n",
        "\n",
        "  else:\n",
        "    raise Exception('Error! '+ name +' dataset not found...')\n",
        "\n",
        "  train_loader = DataLoader(train, batch_size= batch_size, shuffle=shuffle, drop_last = True)\n",
        "  test_loader = DataLoader(test, batch_size= batch_size, shuffle=shuffle, drop_last = True)\n",
        "\n",
        "  return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThWOqbh5ZfDN"
      },
      "source": [
        "# Building SNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGBuN5hW39kS"
      },
      "source": [
        "## Initializing The Network"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<details>\n",
        "<summary> Show Code Block </summary>\n",
        "```\n",
        "class SNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # First convolutional block\n",
        "    self.cb_01 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels = 1, out_channels = 12, kernel_size = 5),\n",
        "        nn.GroupNorm(num_groups = 6, num_channels= 12),\n",
        "        snn.Leaky(beta = 0.5, spike_grad = surrogate.fast_sigmoid(slope = 25), init_hidden = True),\n",
        "    )\n",
        "\n",
        "    # Second convolutional block\n",
        "    self.cb_02 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels = 12, out_channels = 64, kernel_size = 5),\n",
        "        nn.GroupNorm(num_groups = 8, num_channels = 64),\n",
        "        snn.Leaky(beta = 0.5, spike_grad = surrogate.fast_sigmoid(slope = 25), init_hidden = True),\n",
        "    )\n",
        "\n",
        "    # Average pooling layer\n",
        "    self.pooling = nn.AvgPool2d(3)\n",
        "\n",
        "    # Third convolutional block\n",
        "    self.cb_03 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 5),\n",
        "        nn.GroupNorm(num_groups = 8, num_channels = 128),\n",
        "        snn.Leaky(beta = 0.5, spike_grad = surrogate.fast_sigmoid(slope = 25), init_hidden = True),\n",
        "    )\n",
        "\n",
        "    # Fully Connected Layer\n",
        "    self.FC = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features = 128 * 2 * 2, out_features= 10),\n",
        "        snn.Leaky(beta = 0.5, spike_grad = surrogate.fast_sigmoid(slope = 25), init_hidden = True, output=True)\n",
        "    )\n",
        "    \n",
        "  def forward(self, x):\n",
        "    # Forward pass through the network\n",
        "    x = self.cb_01(x)\n",
        "    x = self.cb_02(x)\n",
        "    x = self.pooling(x)\n",
        "    x = self.cb_03(x)\n",
        "    x = self.FC(x)\n",
        "\n",
        "    return x\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "TcX4NvIsxArl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using gpu if available\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "Y6PiDfncsNWa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mnist Architecture"
      ],
      "metadata": {
        "id": "P0NEiP-heStc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "M0bnQscP90CW"
      },
      "outputs": [],
      "source": [
        "mnist_net = nn.Sequential(\n",
        "  # First convolution block\n",
        "  nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size = 7),\n",
        "  nn.GroupNorm(num_groups = 8, num_channels= 32),\n",
        "  snn.Leaky(beta = 0.5, spike_grad = surrogate.fast_sigmoid(slope = 25), init_hidden = True),\n",
        "\n",
        "  # Pooling\n",
        "  nn.AvgPool2d(2),\n",
        "\n",
        "  #Second convolution block\n",
        "  nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 4),\n",
        "  nn.GroupNorm(num_groups = 8, num_channels = 64),\n",
        "  snn.Leaky(beta = 0.5, spike_grad = surrogate.fast_sigmoid(slope = 25), init_hidden = True),\n",
        "\n",
        "  # Pooling\n",
        "  nn.AvgPool2d(2),\n",
        "\n",
        "  # Fully connected or output block\n",
        "  nn.Flatten(),\n",
        "  nn.Linear(in_features = 64 * 4 * 4, out_features= 10),\n",
        "  snn.Leaky(beta = 0.5, spike_grad = surrogate.fast_sigmoid(slope = 25), init_hidden = True, output=True)\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CIFAR-10"
      ],
      "metadata": {
        "id": "p5-8I_deitiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cifar_10 = nn.Sequential(\n",
        "  # First convolution block\n",
        "  nn.Conv2d(in_channels = 1, out_channels = 64, kernel_size = 3, stride = 1),\n",
        "  nn.GroupNorm(num_groups = 8, num_channels= 64),\n",
        "  snn.Leaky(beta = 0.5, spike_grad = surrogate.fast_sigmoid(slope = 25), init_hidden = True),\n",
        "\n",
        "  # Second convolution block\n",
        "  nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3, stride = 1),\n",
        "  nn.GroupNorm(num_groups = 8, num_channels= 64),\n",
        "  snn.Leaky(beta = 0.5, spike_grad = surrogate.fast_sigmoid(slope = 25), init_hidden = True),\n",
        "\n",
        "  # Pooling\n",
        "  nn.AvgPool2d(5),\n",
        "\n",
        "  # Third convolution block\n",
        "  nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 3, stride = 1),\n",
        "  nn.GroupNorm(num_groups = 8, num_channels= 128),\n",
        "  snn.Leaky(beta = 0.5, spike_grad = surrogate.fast_sigmoid(slope = 25), init_hidden = True),\n",
        "\n",
        "  # Forth convolution block\n",
        "  nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = 3, stride = 1),\n",
        "  nn.GroupNorm(num_groups = 8, num_channels= 128),\n",
        "  snn.Leaky(beta = 0.5, spike_grad = surrogate.fast_sigmoid(slope = 25), init_hidden = True),\n",
        "\n",
        "  # Pooling\n",
        "  nn.AvgPool2d(5),\n",
        "\n",
        "  # Fifth convolution block\n",
        "  nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = 3, stride = 1),\n",
        "  nn.GroupNorm(num_groups = 8, num_channels= 128),\n",
        "  snn.Leaky(beta = 0.5, spike_grad = surrogate.fast_sigmoid(slope = 25), init_hidden = True),\n",
        "\n",
        "  # Sixth convolution block\n",
        "  nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = 3, stride = 1),\n",
        "  nn.GroupNorm(num_groups = 8, num_channels= 128),\n",
        "  snn.Leaky(beta = 0.5, spike_grad = surrogate.fast_sigmoid(slope = 25), init_hidden = True),\n",
        "\n",
        "  # Pooling\n",
        "  nn.AvgPool2d(5),\n",
        "\n",
        "  # Seventh convolution block\n",
        "  nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 3, stride = 1),\n",
        "  nn.GroupNorm(num_groups = 8, num_channels= 256),\n",
        "  snn.Leaky(beta = 0.5, spike_grad = surrogate.fast_sigmoid(slope = 25), init_hidden = True),\n",
        "\n",
        "  # eighth convolution block\n",
        "  nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, stride = 1),\n",
        "  nn.GroupNorm(num_groups = 8, num_channels= 256),\n",
        "  snn.Leaky(beta = 0.5, spike_grad = surrogate.fast_sigmoid(slope = 25), init_hidden = True),\n",
        "\n",
        "  # Pooling\n",
        "  nn.AvgPool2d(4),\n",
        "\n",
        "  # Fully connected or output block\n",
        "  nn.Flatten(),\n",
        "  nn.Linear(in_features = 256 * 4 * 4, out_features= 10),\n",
        "  snn.Leaky(beta = 0.5, spike_grad = surrogate.fast_sigmoid(slope = 25), init_hidden = True, output=True)\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "PNAIigCzeeFb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CIFAR-10-DVS"
      ],
      "metadata": {
        "id": "UEQWs0_1lNWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cifar_10_dvs = nn.Sequential(\n",
        "  # First convolution block\n",
        "  nn.Conv2d(in_channels = 1, out_channels = 64, kernel_size = 3, stride = 1),\n",
        "  nn.GroupNorm(num_groups = 8, num_channels= 64),\n",
        "  snn.Leaky(beta = 0.5, spike_grad = surrogate.fast_sigmoid(slope = 25), init_hidden = True),\n",
        "\n",
        "  # Second convolution block\n",
        "  nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3, stride = 1),\n",
        "  nn.GroupNorm(num_groups = 8, num_channels= 64),\n",
        "  snn.Leaky(beta = 0.5, spike_grad = surrogate.fast_sigmoid(slope = 25), init_hidden = True),\n",
        "\n",
        "  # Pooling\n",
        "  nn.AvgPool2d(4),\n",
        "\n",
        "  # Third convolution block\n",
        "  nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 3, stride = 1),\n",
        "  nn.GroupNorm(num_groups = 8, num_channels= 128),\n",
        "  snn.Leaky(beta = 0.5, spike_grad = surrogate.fast_sigmoid(slope = 25), init_hidden = True),\n",
        "\n",
        "  # Pooling\n",
        "  nn.AvgPool2d(4),\n",
        "\n",
        "  # Forth convolution block\n",
        "  nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 3, stride = 1),\n",
        "  nn.GroupNorm(num_groups = 8, num_channels= 256),\n",
        "  snn.Leaky(beta = 0.5, spike_grad = surrogate.fast_sigmoid(slope = 25), init_hidden = True),\n",
        "\n",
        "  # Fifth convolution block\n",
        "  nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, stride = 1),\n",
        "  nn.GroupNorm(num_groups = 8, num_channels= 256),\n",
        "  snn.Leaky(beta = 0.5, spike_grad = surrogate.fast_sigmoid(slope = 25), init_hidden = True),\n",
        "\n",
        "  # Pooling\n",
        "  nn.AvgPool2d(4),\n",
        "\n",
        "  # Sixth convolution block\n",
        "  nn.Conv2d(in_channels = 256, out_channels = 512, kernel_size = 3, stride = 1),\n",
        "  nn.GroupNorm(num_groups = 8, num_channels= 512),\n",
        "  snn.Leaky(beta = 0.5, spike_grad = surrogate.fast_sigmoid(slope = 25), init_hidden = True),\n",
        "\n",
        "  # Pooling\n",
        "  nn.AvgPool2d(4),\n",
        "\n",
        "   # Seventh convolution block\n",
        "  nn.Conv2d(in_channels = 512, out_channels = 1024, kernel_size = 3, stride = 1),\n",
        "  nn.GroupNorm(num_groups = 8, num_channels= 1024),\n",
        "  snn.Leaky(beta = 0.5, spike_grad = surrogate.fast_sigmoid(slope = 25), init_hidden = True),\n",
        "\n",
        "  # Fifth convolution block\n",
        "  nn.Conv2d(in_channels = 1024, out_channels = 1024, kernel_size = 3, stride = 1),\n",
        "  nn.GroupNorm(num_groups = 8, num_channels= 1024),\n",
        "  snn.Leaky(beta = 0.5, spike_grad = surrogate.fast_sigmoid(slope = 25), init_hidden = True),\n",
        "\n",
        "  # Pooling\n",
        "  nn.AvgPool2d(4),\n",
        "\n",
        "  # Fully connected or output block\n",
        "  nn.Flatten(),\n",
        "  nn.Linear(in_features = 1024 * 4 * 4, out_features= 10),\n",
        "  snn.Leaky(beta = 0.5, spike_grad = surrogate.fast_sigmoid(slope = 25), init_hidden = True, output=True)\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "eZec_T5jkLNo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHVA8FTp37lq"
      },
      "source": [
        "## Training the Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aQRtO_0V4H3Z"
      },
      "outputs": [],
      "source": [
        "# tranformer\n",
        "transform = transforms.Compose([\n",
        "  transforms.Resize(size = (28, 28)),\n",
        "  transforms.Grayscale(),\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize((0,), (1,))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_metrics(train_acc, train_loss, test_acc, test_loss):\n",
        "    # Set Seaborn style\n",
        "    sns.set(style=\"whitegrid\")\n",
        "\n",
        "    # Create figure and axes\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n",
        "\n",
        "    # Plot training accuracy\n",
        "    sns.lineplot(x=range(len(train_acc)), y=train_acc, ax=axes[0], label='Train Accuracy')\n",
        "    # Plot testing accuracy\n",
        "    sns.lineplot(x=range(len(test_acc)), y=test_acc, ax=axes[0], label='Test Accuracy')\n",
        "\n",
        "    # Plot training loss\n",
        "    sns.lineplot(x=range(len(train_loss)), y=train_loss, ax=axes[1], label='Train Loss')\n",
        "    # Plot testing loss\n",
        "    sns.lineplot(x=range(len(test_loss)), y=test_loss, ax=axes[1], label='Test Loss')\n",
        "\n",
        "    # Set titles and labels\n",
        "    axes[0].set_title('Accuracy')\n",
        "    axes[1].set_title('Loss')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "\n",
        "    # Show legend\n",
        "    axes[0].legend()\n",
        "    axes[1].legend()\n",
        "\n",
        "    # Show plot\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "uZDKlnKfgpOY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_UNW4dmUDroq"
      },
      "outputs": [],
      "source": [
        "def fwrd_pass(net, num_steps, data):\n",
        "  mem_rec = []\n",
        "  spk_rec = []\n",
        "  utils.reset(net)  # resets hidden states for all LIF neurons in net\n",
        "\n",
        "  for step in range(num_steps):\n",
        "      spk_out, mem_out = net(data)\n",
        "      spk_rec.append(spk_out)\n",
        "      mem_rec.append(mem_out)\n",
        "\n",
        "  return torch.stack(spk_rec), torch.stack(mem_rec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0h2H5X5M_nOA"
      },
      "outputs": [],
      "source": [
        "def train_network(net = mnist_net, dataset_name = 'mnist', epoches = 1, lr = 1e-5, betas=(0.9, 0.999), transform = transform, batch_size = 128, shuffle = True):\n",
        "  # Tracking training and evaluating record\n",
        "  test_acc_hist = []\n",
        "  train_acc_hist = []\n",
        "\n",
        "  test_loss_hist = []\n",
        "  train_loss_hist = []\n",
        "\n",
        "\n",
        "  loss = SF.ce_rate_loss()\n",
        "  optimizer = torch.optim.AdamW(net.parameters(), lr = lr, betas = betas)\n",
        "\n",
        "  # Fetching dataset\n",
        "  train_loader, test_loader = fetch_static_data( name = dataset_name, transform = transform, batch_size = batch_size, shuffle = shuffle)\n",
        "  train_loader_len, test_loader_len = len(train_loader), len(test_loader)\n",
        "\n",
        "\n",
        "  clear_output()\n",
        "  # Creating Training and evaluation loop\n",
        "  for epoch in range(epoches):\n",
        "    print('-'*10, 'ITERATION', epoch, '-'*10, )\n",
        "\n",
        "    # Training model\n",
        "    net.train()\n",
        "    acc = 0\n",
        "    total = 0\n",
        "    print('Training...')\n",
        "    for batch_no, (X, y) in enumerate(train_loader):\n",
        "      # sending the data to same device\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      # forward pass\n",
        "      spk_rec, _ = fwrd_pass(net, 10, X)\n",
        "\n",
        "      # initialize the loss & sum over time\n",
        "      loss_val = loss(spk_rec, y)\n",
        "\n",
        "      # Gradient calculation + weight update\n",
        "      optimizer.zero_grad()\n",
        "      loss_val.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # calculating accuracy\n",
        "      acc += SF.accuracy_rate(spk_rec, y) * spk_rec.size(1)\n",
        "      total += spk_rec.size(1)\n",
        "\n",
        "      # showing progress\n",
        "      if (batch_no+1) % (train_loader_len//10) == 0:\n",
        "        train_acc = acc/total\n",
        "        print(f'{(batch_no // (train_loader_len//10)+1)*10}% training completed with {train_acc * 100:.2f}% accuracy and {loss_val.item():.2f} loss')\n",
        "\n",
        "        # Store accuracy and loss history for future plotting\n",
        "        train_loss_hist.append(loss_val.item())\n",
        "        train_acc_hist.append(train_acc.item()*100)\n",
        "\n",
        "\n",
        "    # Evaluating model\n",
        "    print('Testing...')\n",
        "    net.eval\n",
        "    with torch.inference_mode():\n",
        "      acc = 0\n",
        "      total = 0\n",
        "      for batch_no, (X, y) in enumerate(test_loader):\n",
        "        # sending the data to same device\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # forward pass\n",
        "        spk_rec, _ = fwrd_pass(net, 10, X)\n",
        "\n",
        "        # initialize the loss & sum over time\n",
        "        loss_val = loss(spk_rec, y)\n",
        "\n",
        "\n",
        "        # calculating accuracy\n",
        "        acc += SF.accuracy_rate(spk_rec, y) * spk_rec.size(1)\n",
        "        total += spk_rec.size(1)\n",
        "\n",
        "        # showing progress\n",
        "        if (batch_no+1) % (test_loader_len//10) == 0:\n",
        "          test_acc = acc/total\n",
        "          print(f'{(batch_no // round(test_loader_len/10)+1)*10}% testing completed with {test_acc * 100:.2f}%  accuracy and {loss_val.item():.2f} loss')\n",
        "\n",
        "          # Store accuracy and loss history for future plotting\n",
        "          test_loss_hist.append(loss_val.item())\n",
        "          test_acc_hist.append(test_acc.item()*100)\n",
        "\n",
        "  return {\n",
        "      'net': net,\n",
        "      'train accuracy' : train_acc_hist,\n",
        "      'test accuracy': test_acc_hist,\n",
        "      'train loss': train_loss_hist,\n",
        "      'test loss': test_loss_hist\n",
        "      }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary(mnist_net, (1, 28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1hT0XaynYWI",
        "outputId": "541d3831-0e49-4529-c013-edef4ea4dff3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 22, 22]           1,600\n",
            "         GroupNorm-2           [-1, 32, 22, 22]              64\n",
            "             Leaky-3           [-1, 32, 22, 22]               0\n",
            "         AvgPool2d-4           [-1, 32, 11, 11]               0\n",
            "            Conv2d-5             [-1, 64, 8, 8]          32,832\n",
            "         GroupNorm-6             [-1, 64, 8, 8]             128\n",
            "             Leaky-7             [-1, 64, 8, 8]               0\n",
            "         AvgPool2d-8             [-1, 64, 4, 4]               0\n",
            "           Flatten-9                 [-1, 1024]               0\n",
            "           Linear-10                   [-1, 10]          10,250\n",
            "            Leaky-11       [[-1, 10], [-1, 10]]               0\n",
            "================================================================\n",
            "Total params: 44,874\n",
            "Trainable params: 44,874\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.49\n",
            "Params size (MB): 0.17\n",
            "Estimated Total Size (MB): 0.67\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_history = train_network(net = copy(mnist_net), batch_size = 512, epoches=20)\n",
        "\n",
        "plot_metrics(mnist_history['train accuracy'], mnist_history['train loss'], mnist_history['test accuracy'], mnist_history['test loss'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylhH1DYgj-YI",
        "outputId": "5c7f76af-ee4f-4068-ef14-be3106d4cf32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- ITERATION 0 ----------\n",
            "Training...\n",
            "10% training completed with 9.46% accuracy and 2.30 loss\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary(mnist_net, (1, 28, 28))"
      ],
      "metadata": {
        "id": "sL_RSqCjniy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fashion_mnist_history = train_network(net = copy(mnist_net),  dataset_name = 'fashion-mnist', batch_size = 512, , epoches=20)\n",
        "\n",
        "plot_metrics(fashion_mnist_history['train accuracy'], fashion_mnist_history['train loss'], fashion_mnist_history['test accuracy'], fashion_mnist_history['test loss'])"
      ],
      "metadata": {
        "id": "pB1ExdtRfem1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(mnist_net, (1, 28, 28))"
      ],
      "metadata": {
        "id": "eBR2fIBmnjYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrNVSJpaFE2b"
      },
      "outputs": [],
      "source": [
        "cifar_10 = train_network(net = copy(mnist_net),  dataset_name = 'cifar-10', batch_size = 512, epoches=20)\n",
        "\n",
        "plot_metrics(cifar_10['train accuracy'], cifar_10['train loss'], cifar_10['test accuracy'], cifar_10['test loss'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l."
      ],
      "metadata": {
        "id": "EHDaNjfyn1Uw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "xV4NAFuU3sAr"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}